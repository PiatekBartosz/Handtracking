import depthai as dai
from pathlib import Path
from collections import namedtuple
import cv2
import numpy as np

pipeline = dai.Pipeline()

SCRIPT_DIR = Path(__file__).resolve().parent
PALM_DETECTION_MODEL = str(SCRIPT_DIR / "models/palm_detection_sh4.blob")
PD_INPUT_SIZE = 128
pd_score_thresh = 0.5
pd_nms_thresh = 0.3

SSDAnchorOptions = namedtuple('SSDAnchorOptions',[
        'num_layers',
        'min_scale',
        'max_scale',
        'input_size_height',
        'input_size_width',
        'anchor_offset_x',
        'anchor_offset_y',
        'strides',
        'aspect_ratios',
        'reduce_boxes_in_lowest_layer',
        'interpolated_scale_aspect_ratio',
        'fixed_anchor_size'])

# Define source
color = pipeline.createColorCamera()
pd_nn = pipeline.createNeuralNetwork()

# Define output

# Video stream
xoutVideo = pipeline.createXLinkOut()
# Hand detection (open/closed + location)
xoutPD = pipeline.createXLinkOut()


# Name streams
xoutVideo.setStreamName("video")
xoutPD.setStreamName("palm detection")

# Properties
color.setPreviewSize(128, 128)
color.setBoardSocket(dai.CameraBoardSocket.RGB)
color.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
color.setInterleaved(False)
color.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
color.setPreviewSize(PD_INPUT_SIZE, PD_INPUT_SIZE)

# Neural Network properties
pd_nn.setBlobPath(PALM_DETECTION_MODEL)

# nn_setup
# anchors, anchors_count = postprocess.create_SSD_anchors(PD_INPUT_SIZE)


# Link nn
color.preview.link(pd_nn.input)
pd_nn.out.link(xoutPD.input)


# Linking RGB
color.video.link(xoutVideo.input)
# color.video.link(resize)



def decode_bboxes(score_thresh, wi, hi, scores, bboxes, anchors):
    """
    wi, hi : NN input shape
    mediapipe/calculators/tflite/tflite_tensors_to_detections_calculator.cc
    # Decodes the detection tensors generated by the model, based on
    # the SSD anchors and the specification in the options, into a vector of
    # detections. Each detection describes a detected object.
    https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection_cpu.pbtxt :
    node {
        calculator: "TensorsToDetectionsCalculator"
        input_stream: "TENSORS:detection_tensors"
        input_side_packet: "ANCHORS:anchors"
        output_stream: "DETECTIONS:unfiltered_detections"
        options: {
            [mediapipe.TensorsToDetectionsCalculatorOptions.ext] {
            num_classes: 1
            num_boxes: 896
            num_coords: 18
            box_coord_offset: 0
            keypoint_coord_offset: 4
            num_keypoints: 7
            num_values_per_keypoint: 2
            sigmoid_score: true
            score_clipping_thresh: 100.0
            reverse_output_order: true
            x_scale: 128.0
            y_scale: 128.0
            h_scale: 128.0
            w_scale: 128.0
            min_score_thresh: 0.5
            }
        }
    }
    """
    sigmoid_scores = 1 / (1 + np.exp(-scores))
    regions = []
    for i,anchor in enumerate(anchors):
        score = sigmoid_scores[i]

        if score > score_thresh:
            # If reverse_output_order is true, sx, sy, w, h = bboxes[i,:4]
       def calculate_scale(min_scale, max_scale, stride_index, num_strides):
    if num_strides == 1:
        return (min_scale + max_scale) / 2
    else:
        return min_scale + (max_scale - min_scale) * stride_index / (num_strides - 1)     # Here reverse_output_order is true

            sx, sy, w, h = bboxes[i,:4]
            cx = sx * anchor.w / wi + anchor.x_center
            cy = sy * anchor.h / hi + anchor.y_center
            w = w * anchor.w / wi
            h = h * anchor.h / hi
            box = [cx - w*0.5, cy - h*0.5, w, h]

            kps = {}
            # 0 : wrist
            # 1 : index finger joint
            # 2 : middle finger joint
            # 3 : ring finger joint
            # 4 : little finger joint
            # 5 :
            # 6 : thumb joint
            for j, name in enumerate(["0", "1", "2", "3", "4", "5", "6"]):
                # Here reverse_output_order is true
                lx, ly = bboxes[i,4+j*2:6+j*2]
                lx = lx * anchor.w / wi + anchor.x_center
                ly = ly * anchor.h / hi + anchor.y_center
                kps[name] = [lx, ly]
            regions.append(HandRegion(float(score), box, kps))
    return regions


with dai.Device(pipeline) as device:

    # output queue
    video = device.getOutputQueue('video')
    pd = device.getOutputQueue('palm detection')

    bboxes = []

    while True:
        videoFrame = video.get()
        nn_output = pd.get()

        if nn_output is not None:

            # nn output consists of classificators and regressors
            scores = np.array(nn_output.getLayerFp16("classificators"))
            bboxes = np.array(nn_output.getLayerFp16("regressors")).reshape((896, 18))
            bboxes_decoded = decode_bboxes(bboxes)

        # Get BGR frame from NV12 encoded video frame to show with opencv
        cv2.imshow("video", videoFrame.getCvFrame())

        if cv2.waitKey(1) == ord('q'):
            break